{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Year Project\n",
    "\n",
    "## Project Description\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Std\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import json\n",
    "import platform\n",
    "from IPython.display import display as d\n",
    "\n",
    "# Array manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Statistics\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Plots\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Visual\n",
    "import shapely\n",
    "from shapely.geometry import Point, MultiPoint, LineString, MultiLineString, Polygon, MultiPolygon\n",
    "\n",
    "import folium\n",
    "from folium import plugins\n",
    "from folium.plugins import HeatMap, MarkerCluster\n",
    "\n",
    "%run -i ../scripts/functions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging levels\n",
    "LOG = True\n",
    "\n",
    "# Data paths\n",
    "PATH = {\n",
    "    'raw': Path('../data/raw/'),\n",
    "    'processed': Path('../data/processed/'),\n",
    "    'interim': Path('../data/interim/'),\n",
    "    'references': Path('../references')\n",
    "}\n",
    "\n",
    "# Raw paths\n",
    "PATH['raw_accident'] = PATH['raw'] / \"Road Safety Data - Accidents 2019.csv\"\n",
    "PATH['raw_casual'] = PATH['raw'] / \"Road Safety Data - Casualties 2019.csv\"\n",
    "PATH['raw_vehicles'] = PATH['raw'] / \"Road Safety Data- Vehicles 2019.csv\"\n",
    "\n",
    "# Interim paths\n",
    "PATH['accident'] = PATH['interim'] / \"bradford_accidents.csv\"\n",
    "PATH['casual'] = PATH['interim'] / \"bradford_casualties.csv\"\n",
    "PATH['vehicles'] = PATH['interim'] / \"bradford_vehicles.csv\"\n",
    "\n",
    "# Ref paths\n",
    "PATH['var_lookup'] = PATH['references'] / 'variable lookup.xls'\n",
    "\n",
    "\n",
    "EXCEL_LABELS = pd.read_excel(PATH['var_lookup'], sheet_name = None, index_col = 0)\n",
    "VAR = {}\n",
    "\n",
    "# maps special sheet name to full name for later proccesing\n",
    "special_sheet_names = {'Ped_Location': 'Pedestrian_Location', \\\n",
    "                  'Ped_Movement': 'Pedestrian_Movement',\n",
    "                      'Speed_Limit': 'Speed_limit'}\n",
    "\n",
    "for sheet_name in EXCEL_LABELS:\n",
    "    # add underscores in field name since its missing in excel\n",
    "    fixed_sheet_name = sheet_name.replace(\" \", \"_\")\n",
    "    \n",
    "    # handle sheet names that are significally different named compared to the table headers\n",
    "    if fixed_sheet_name in special_sheet_names:\n",
    "        fixed_sheet_name = special_sheet_names[fixed_sheet_name]\n",
    "    \n",
    "    # create new dict with renamed keys\n",
    "    VAR[fixed_sheet_name] = EXCEL_LABELS[sheet_name]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Special function for labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_label_fld_lookup(data=None, colname=None):\n",
    "    '''Returns a DataFrame with only the labels needed to set xticklabels\n",
    "    When settting xticklabels, we often have more labels in VAR than present in our dataset.\n",
    "    in a way, this function creates a copy of VAR with only the labels used in our data (bradford)\n",
    "    \n",
    "    :param dataset: A Pandas DataFrame, eg DATA['accident']\n",
    "    :param column: Column name as string, with underscores\n",
    "    '''\n",
    "    # problem: not all labels are used in 'Special Conditions at Site'\n",
    "    # solution: remove the labelse not present from VAR['Special Conditions at Site']\n",
    "    # used_label_codes = pd.unique(DATA['accident']['Special_Conditions_at_Site'])\n",
    "    used_label_codes = pd.unique(data[colname])\n",
    "\n",
    "    # Save all labels for this column.\n",
    "    all_labels = VAR[colname]\n",
    "\n",
    "    # set default values of the relevant labels we want to keep\n",
    "    unique_label_fld_lookup = all_labels\n",
    "\n",
    "    # loop through all label codes (they are set as the index)\n",
    "    for label_code in all_labels.index:\n",
    "        if label_code not in used_label_codes:\n",
    "            # if the label is not used \n",
    "            unique_label_fld_lookup.drop(index=label_code, inplace=True)\n",
    "\n",
    "    return unique_label_fld_lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 0 - Data filtering and cleaning\n",
    "\n",
    "Most of our data filtering and cleaning happends from scripts we wrote,\n",
    "which takes in the raw files as input and process them to only show bradford.\n",
    "\n",
    "They can be run from the terminal or here. It's in src/scripts.\n",
    "\n",
    "Pandas can handle and process datasets even though they're missing.\n",
    "We're categorizing missing values as N/A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process raw files into bradford files with pandas(pd)\n",
    "# Depends on paths from this notebook, so run for here or alter to be independant.\n",
    "%run -i ../src/scripts/process_bradford_in.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = {}\n",
    "\n",
    "# Read data and explicitly define some data types for columns.\n",
    "# The datasets are saved in utf8 with bom, so we need utf-8-sig encoding.\n",
    "# We treat explicit -1 from the datasets as NA because it's their definition from the documentation.\n",
    "\n",
    "DATA['accident'] = pd.read_csv(PATH['accident'], dtype={0: 'string', 31: 'string'}, encoding='utf-8-sig', na_values=\"-1\")\n",
    "DATA['casual'] = pd.read_csv(PATH['casual'], dtype={0: 'string'}, encoding='utf-8-sig', na_values=\"-1\")\n",
    "DATA['vehicles'] = pd.read_csv(PATH['vehicles'], dtype={0: 'string'}, encoding='utf-8-sig', na_values=\"-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values overview\n",
    "\n",
    "# Quick overview\n",
    "DATA['accident'].isna().any()\n",
    "\n",
    "# Get only the names of the columns with missing values: Time, Junction_Control and 2nd_Road_Class\n",
    "DATA['accident'].columns[DATA['accident'].isna().any()]\n",
    "\n",
    "# Pandas: first sum the missing values row-wise, then sum all the row-wise sums to one single sum\n",
    "# Missing                            Not missing\n",
    "DATA['accident'].isna().sum().sum(), DATA['accident'].notna().sum().sum()\n",
    "\n",
    "# Return the rows with missing values\n",
    "# We use d() to display multiple elements\n",
    "d(DATA['accident'][DATA['accident']['Time'].isna()])\n",
    "d(DATA['accident'][DATA['accident']['Junction_Control'].isna()]['Junction_Control'])\n",
    "d(DATA['accident'][DATA['accident']['2nd_Road_Class'].isna()]['Junction_Control'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - Single variable analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age analysis\n",
    "pd.concat([VAR['Age_Band'], DATA['casual']['Age_Band_of_Casualty'].value_counts()], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Age=VAR['Age_Band']['label'][:11]\n",
    "Numbers=DATA['casual']['Age_Band_of_Casualty'].value_counts(sort=False)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(7, 3))\n",
    "axes = fig.add_axes([0, 0, 1, 1])\n",
    "plt.bar(Age, Numbers,width=0.5, color='blue', edgecolor='black')\n",
    "axes.set_ylabel('Number of Casualties');axes.set_xlabel('Age Range');axes.set_title('Number of Casualties regarding their ages');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time analysis\n",
    "pd.concat([VAR['Day_of_Week'], DATA['accident']['Day_of_Week'].value_counts()], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Weekdays=VAR['Day_of_Week']['label']\n",
    "Numbers=DATA['accident']['Day_of_Week'].value_counts(sort=False)\n",
    "    \n",
    "    \n",
    "fig = plt.figure(figsize=(6, 3))\n",
    "axes = fig.add_axes([0, 0, 1, 1])\n",
    "plt.bar(Weekdays, Numbers,width=0.5, color='blue', edgecolor='black')\n",
    "axes.set_ylabel('Number of Accidents');axes.set_xlabel('Days of the Week');axes.set_title('Total number of Accidents regarding days of the week');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Movement analysis - Task 1, c\n",
    "pd.concat([VAR['Pedestrian_Movement'], DATA['casual']['Pedestrian_Movement'].value_counts()], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PedMove=VAR['Pedestrian_Movement']['label'][:10]\n",
    "Numbers=DATA['casual']['Pedestrian_Movement'].value_counts(sort=False)\n",
    "\n",
    "fig = plt.figure(figsize=(6, 3))\n",
    "axes = fig.add_axes([0, 0, 1, 1])\n",
    "plt.barh(PedMove, Numbers, color='blue', edgecolor='black')\n",
    "axes.set_xlabel('Number of Casualties');axes.set_title('Number of Casualties regarding the movement of the Pedestrian');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Associations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to see if there's an association between the casualty severity from the casualties table, and a field from the accident table. We focus on casualty serverity associatied with a road type or road charecteristic.  \n",
    "\n",
    "Casualty serverity vs\n",
    "- Road Type\n",
    "- Junction Detail\n",
    "- Speed limit\n",
    "\n",
    "We choose these since they are deemed best suited to shed light on our research question. The dataset are full of categorical varibles, including the ones above, so we choose to test for statistical assocaition using Pearson $\\chi^2$ test of independence. See references for more information on this test\n",
    "\n",
    "Every single casualty was part of an accident. So in order to get fields from diffrent tables we do an inner join to get the extra information about the casualty. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = DATA['casual'].merge(DATA['accident'], how=\"inner\")\n",
    "merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Association test between Casualty_Severity and Road_Type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make contingency table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observed_pd1 = pd.crosstab(merged[\"Road_Type\"], merged[\"Casualty_Severity\"])\n",
    "observed1 = observed_pd1.to_numpy()\n",
    "observed_pd1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove the road types with almost no accidents since we are not intersted in them, if there's no casualties happening.  \n",
    "The Pearson chi-square test should only be used if most cells have an expected count above 5, and the minimum expected count is at least 1.  \n",
    "Since we have 3 columns we say that we want the rows to sum to least $5*3 = 15$, so we obmit some rows using a mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mask\n",
    "observed_pd1_mask = observed_pd1.sum(axis=1) >= 15\n",
    "# use mask to change data\n",
    "observed_pd1_filtered = observed_pd1[observed_pd1_mask]\n",
    "observed1 = observed_pd1_filtered.to_numpy()\n",
    "observed_pd1_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare these observed values with expected values. This will tell there's an association between the variables.\n",
    "We calculate cramers V to see how strong the assocation is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chiVal1, pVal1, df1, observed1, expected1, V1 = get_pearson_test(observed=observed1)\n",
    "chiVal1, pVal1, df1, observed1, expected1.astype(int), V1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create labels and remove those not needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var1_labels1 = dict(get_unique_label_fld_lookup(data=merged, colname=\"Road_Type\").label)\n",
    "var2_labels1 = list(VAR[\"Casualty_Severity\"].label)\n",
    "var1_labels1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelcodestoremove1 = []\n",
    "for labelcode in var1_labels1:\n",
    "    if labelcode not in observed_pd1_filtered.index:\n",
    "        labelcodestoremove1.append(labelcode)\n",
    "        \n",
    "for label in labelcodestoremove1:\n",
    "    var1_labels1.pop(label)\n",
    "var1_labels1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title1 = \"Pearson test of independence: Casualty severity vs Road Type\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pearson_test(observed=observed1, expected=expected1, var1_labels=var1_labels1, var2_labels=var2_labels1, title=title1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Association test between Casualty_Severity vs Junction_Detail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same proceduce is done below for the two other fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observed_pd2 = pd.crosstab(merged[\"Junction_Detail\"], merged[\"Casualty_Severity\"], margins=False)\n",
    "observed2 = observed_pd2.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mask\n",
    "observed_pd2_mask = observed_pd2.sum(axis=1) >= 15\n",
    "# use make to change data\n",
    "observed_pd2_filtered = observed_pd2[observed_pd2_mask]\n",
    "observed2 = observed_pd2_filtered.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var1_labels2 = dict(get_unique_label_fld_lookup(data=merged, colname=\"Junction_Detail\").label)\n",
    "var2_labels2 = list(VAR[\"Casualty_Severity\"].label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelcodestoremove = []\n",
    "for labelcode in var1_labels2:\n",
    "    if labelcode not in observed_pd2_filtered.index:\n",
    "        labelcodestoremove.append(labelcode)\n",
    "        \n",
    "for label in labelcodestoremove:\n",
    "    var1_labels2.pop(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chiVal2, pVal2, df2, observed2, expected2, V2 = get_pearson_test(observed=observed2)\n",
    "chiVal2, pVal2, df2, observed2, expected2.astype(int), V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title2 = \"Pearson test of independence: Casualty severity vs Junction detail \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pearson_test(observed=observed2, expected=expected2, var1_labels=var1_labels2, var2_labels=var2_labels2, title=title2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Association test between Casualty_Severity vs Speed_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observed_pd3 = pd.crosstab(merged[\"Speed_limit\"], merged[\"Casualty_Severity\"], margins=False)\n",
    "observed3 = observed_pd3.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mask\n",
    "observed_pd3_mask = observed_pd3.sum(axis=1) >= 15\n",
    "# use make to change data\n",
    "observed_pd3_filtered = observed_pd3[observed_pd3_mask]\n",
    "observed3 = observed_pd3_filtered.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var1_labels3 = dict(get_unique_label_fld_lookup(data=merged, colname=\"Speed_limit\").label)\n",
    "var2_labels3 = list(VAR[\"Casualty_Severity\"].label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelcodestoremove3 = []\n",
    "for labelcode in var1_labels3:\n",
    "    if labelcode not in observed_pd3_filtered.index:\n",
    "        labelcodestoremove3.append(labelcode)\n",
    "        \n",
    "for label in labelcodestoremove3:\n",
    "    var1_labels3.pop(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chiVal3, pVal3, df3, observed3, expected3, V3 = get_pearson_test(observed=observed3)\n",
    "chiVal3, pVal3, df3, observed3, expected3.astype(int), V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title3 = \"Pearson test of independence: Casualty severity vs Speed Limit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pearson_test(observed=observed3, expected=expected3, var1_labels=var1_labels3, var2_labels=var2_labels3, title=title3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Map visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH[\"processed\"] / \"citieslad.json\") as f:\n",
    "    cities_json = json.load(f)\n",
    "\n",
    "# cities_json['Bradford']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latlons = pd.concat([DATA[\"accident\"]['Latitude'], DATA[\"accident\"]['Longitude']], axis=1).values.tolist()\n",
    "\n",
    "centroid = list(MultiPoint(latlons).centroid.coords)[0]\n",
    "\n",
    "m1 = folium.Map(centroid, zoom_start=11)\n",
    "\n",
    "for i, row in DATA[\"accident\"].iterrows():\n",
    "    folium.CircleMarker([row['Latitude'], row['Longitude']],\n",
    "        radius = 5,\n",
    "        popup = row['Accident_Index'] + \"\\n\" + row[\"Date\"] + \", \" + str(row[\"Time\"]),\n",
    "        fill_color = \"#3db7e4\",\n",
    "    ).add_to(m1)\n",
    "\n",
    "HeatMap(latlons).add_to(folium.FeatureGroup(name='Heat Map').add_to(m1))\n",
    "folium.LayerControl().add_to(m1)\n",
    "m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latlons = pd.concat([DATA[\"accident\"][\"Latitude\"], DATA['accident']['Longitude']], axis = 1).values.tolist()\n",
    "\n",
    "# centroid = list(MultiPoint(latlons).centroid.coords)[0]\n",
    "\n",
    "m2 = folium.Map(centroid, zoom_start = 11)\n",
    "\n",
    "marker_cluster = MarkerCluster().add_to(folium.FeatureGroup(name='Clusters').add_to(m2))\n",
    "\n",
    "for i, row in DATA['accident'].iterrows():\n",
    "    folium.CircleMarker([row['Latitude'], row['Longitude']],\n",
    "        radius = 5, \n",
    "        popup = row['Accident_Index'] + '\\n' + row['Date'] + \", \" + str(row['Time']),\n",
    "        fill_color = \"#3db7e4\",\n",
    "    ).add_to(marker_cluster)\n",
    "    \n",
    "    HeatMap(latlons).add_to(folium.FeatureGroup(name='Heat Map').add_to(m2))\n",
    "    folium.LayerControl().add_to(m2)\n",
    "m2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Open question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
